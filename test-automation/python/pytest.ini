[tool:pytest]
# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Minimum pytest version
minversion = 7.0

# Add options
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --html=reports/pytest_report.html
    --self-contained-html
    --alluredir=reports/allure-results
    --maxfail=5
    --durations=10

# Markers for test categorization
markers =
    smoke: Critical functionality tests
    regression: Full regression test suite
    critical: Critical priority tests
    high: High priority tests
    medium: Medium priority tests
    low: Low priority tests
    negative: Negative test scenarios
    positive: Positive test scenarios
    security: Security-related tests
    performance: Performance tests
    api: API integration tests
    ui: UI/Frontend tests
    database: Database tests
    authentication: Authentication tests
    file_management: File management tests
    admin: Admin panel tests
    premium: Premium features tests
    otp: OTP security tests
    webauthn: WebAuthn tests
    blockchain: Blockchain storage tests
    search: Search functionality tests
    navigation: Navigation tests
    validation: Form validation tests
    compatibility: Cross-browser compatibility tests
    mobile: Mobile responsive tests
    slow: Long-running tests
    fast: Quick tests
    integration: Integration tests
    unit: Unit tests
    e2e: End-to-end tests
    smoke_test: Smoke test scenarios
    sanity: Sanity check tests

# Test timeout (in seconds)
timeout = 300

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Warnings configuration
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning

# JUnit XML output
junit_suite_name = SecureDocs Test Suite
junit_logging = all

# Coverage configuration (if using pytest-cov)
# addopts = --cov=pages --cov=utils --cov-report=html:reports/coverage --cov-report=term

# Parallel execution settings
# addopts = -n auto --dist=loadfile

# Test retry configuration
# addopts = --reruns 2 --reruns-delay 1

# Custom test result summary
console_output_style = progress

# xfail behavior
xfail_strict = false

# Collect configuration
collect_ignore = [
    "build",
    "dist", 
    "*.egg",
    ".tox",
    ".git",
    "__pycache__"
]

# Test data directories
testmon_datadir = .testmondata

# Cache directory
cache_dir = .pytest_cache

# Doctest options
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL

# Live logging
log_auto_indent = true

# Assertion rewriting
enable_assertion_pass_hook = true

# Plugin configuration
required_plugins = 
    pytest-html>=3.0.0
    allure-pytest>=2.8.0

# Environment variables for tests
env =
    TEST_ENV = automation
    PYTHONPATH = .

# Test file patterns
norecursedirs = .git .tox build dist *.egg .venv venv

# Timeout for individual tests
timeout_method = thread

# Mock configuration
mock_use_standalone_module = true

# BDD configuration (if using pytest-bdd)
bdd_strict_gherkin = false

# HTML report configuration
html_report_title = SecureDocs Selenium Test Report

# Custom test result codes
exit_codes = 
    0: All tests passed
    1: Tests failed
    2: Test execution interrupted
    3: Internal error
    4: pytest usage error
    5: No tests collected
